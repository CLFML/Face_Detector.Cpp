{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":"<p>Welcome to the Face Detector.Cpp project wiki.</p>"},{"location":"about/implementation/","title":"Library implementation details","text":"<p>Here some brief details as how the library is implemented;</p>"},{"location":"about/implementation/#roi-calculation","title":"ROI Calculation","text":"<p>The BlazeFace model, which is based on the SSD (Single Shot MultiBox Detector) framework, makes extensive use of anchoring. The model outputs 896 boxes, which represent potential locations where faces might be detected. By evaluating classifier scores, we can determine which box has the highest likelihood of containing a face.</p> <p>Faces are detected in one or multiple boxes that fall within a grid composed of multiple layers. In this implementation, two grids of sizes 16x16 and 8x8 are used. When overlapped, they look something like this:</p> <p></p> <p>The green grid represents the 8x8 grid.</p> <p>The blue grid represents the 16x16 grid.</p> <p>But you might ask, 16x16 + 8x8 = 320, which is not equal to 896, right? Yes, you're correct! To account for this, we add layers. Layers allow us to add more grid boxes at the same location, which helps increase detection accuracy by providing multiple scales and aspect ratios of anchors at each grid cell. By adding 2 layers to the 16x16 grid we get more accuracy for smaller faces and by adding 6 layers to the 8x8 grid we get more accuracy for larger faces. Which fortunately for us is the exact 896 boxes which comes out of this model ;)</p> <p>To make things easier this implementation generates these anchors before inference and saves them into an array that directly maps to the model output, besides this it gives the boxes a size of 1x1 which helps with scaling it to our input image.</p>"},{"location":"contributing/license/","title":"License","text":"<p>This work is licensed under the apache-2.0 license.</p>"},{"location":"contributing/rules/","title":"Contribution rules","text":"<p>For contributing we recommend creating a fork and when ready a pull request to merge the changes with this repository.  In the pull request please state:</p> <ul> <li>What has been added/changed?</li> <li>Some reasoning about implementation details</li> </ul> <p>Please don't do refactors without consent of administrators  as we will not merge them."},{"location":"contributing/rules/#new-features","title":"New features","text":"<p>If you have any cool feature/idea to add to the code, please start an issue in GitHub to introduce the idea to the maintainers.</p>"},{"location":"usage/ROS_demo/","title":"ROS2 Demo","text":"<p>The ROS2 Demo demonstrates the functionality of the Face Detector library as a ROS2 package. It uses videoframes from your computer's webcam and tries to detect a face, get the Region of Interest as well as key landmarks. </p>"},{"location":"usage/ROS_demo/#building-and-compiling","title":"Building and compiling","text":"<p>Build the package with more verbose output</p> <pre><code>colcon build --event-handlers console_direct+ --packages-select face_detector\n</code></pre> <p>Note that, whenever build errors occur, and you need to clean the build, use</p> <pre><code>rm -rf build install log\n-rf ~/.ros\n</code></pre>"},{"location":"usage/ROS_demo/#run-the-demo","title":"Run the demo","text":"<p>Before you can use any of the installed executables or libraries, you will need to add them to your path and library paths.  colcon will have generated bash/bat files in the install directory to help set up the environment.  These files will add all of the required elements to your path and library paths as well as provide any bash or shell commands exported by packages.</p> <pre><code>source install/setup.bash\n</code></pre> <p>With the environment sourced, we can run executables built by colcon. Let\u2019s run the camera node from the examples:</p> <pre><code>ros2 run face_detector camera_node\n</code></pre> <p>In another terminal, let\u2019s run the face detector node (don\u2019t forget to source the setup script):</p> <pre><code>ros2 run face_detector face_detector_node\n</code></pre> <p>In yet another terminal, let\u2019s run the viewer node:</p> <pre><code>ros2 run face_detector face_detector_viewer\n</code></pre> <p>Now, you should see the camera stream, with annotations produced by the face detector.</p>"},{"location":"usage/face_roi_demo/","title":"Face Roi Demo","text":"<p>The Face Roi Demo that comes with this library demonstrates the (Region of Interest)ROI functionality of this library. It uses videoframes from your computer's webcam and tries to detect a face and get the Face ROI. </p>"},{"location":"usage/face_roi_demo/#building-and-compiling","title":"Building and compiling","text":"<p>To build and run this example you need all the prerequisites you would normally need to build the library. As these steps differ a lot between platforms/oses, a guide for the three most used platforms was made:</p> <ul> <li>Windows Guide</li> <li>Mac Os Guide</li> <li>Linux Guide</li> </ul>"},{"location":"usage/overview/","title":"Overview","text":"<p>The library offers a relatively simple abstraction (compared to the MediaPipe framework) for use with the BlazeFace Mediapipe model. The code is directly built up on the TensorFlow Lite library and uses basic OpenCV functions to preprocess videoframe's and postprocess the results from the model.</p>"},{"location":"usage/overview/#api","title":"API","text":"<p>The library API consists of the following functions: <pre><code>/**\n* @brief Constructor\n* @param det_threshold The sensitivity of the face detector implementation \n* (default=0.75)\n*/\nFaceDetector(const float det_threshold = 0.75f);\n\n/**\n* @brief Loads model and initializes the inference runtime\n* @param model_path Path to the Mediapipe Blazeface face detector model (.tflite) file\n* @param delegate_type The delegate to use for inference (CPU or TPU)(default=CPU)\n* @param num_of_threads The number of CPU threads which can be used \n*                       by the inference runtime\n*/\nvoid load_model(const std::string model_path, \n                const face_detector_delegate delegate_type = face_detector_delegate::CPU, \n                const uint8_t num_of_threads = 4);\n\n/**\n * @brief Loads image into model and does inference\n * @param frame Any frame which is formatted in CV_8UC3 or CV_8UC4 format\n*/\nvoid load_image(cv::Mat &amp;frame);\n\n/**\n * @brief Gets the Region of Interest, formatted as a square area (scaled to input image) where a detected face might be in\n * @return Rectangular area which contains a detected face\n*/\ncv::Rect get_face_roi();\n\n/**\n * @brief Gets the 2D landmarks from the face.\n * @return Array with 6 Facial landmarks;\n *         Index 0: Left Eye\n *         Index 1: Right Eye\n *         Index 2: Nose\n *         Index 3: Mouth\n *         Index 4: Left ear\n *         Index 5: Right ear\n*/\nstd::array&lt;cv::Point, NUM_OF_FACE_DETECTOR_LANDMARKS&gt; get_face_landmarks();\n\n/**\n * @brief Determine whether a face was detected\n * @return 0 if face was detected, 1 if no face was detected in input frame\n*/\nint detected();\n</code></pre> The comments above the functions describe fairly well what each function does. Here some additional notes;</p> <p>Detector sensitivity</p> <p>The constructor has the option to adjust the sensitivity of the face detector. As this option determines the minimum threshold of the region confidence score output of the model, threshold values closer to 0 will result in higher sensitivity, leading to more false positives. Conversely, a value closer to 1 will result in lower sensitivity, leading to more false negatives.</p> <p>Model path</p> <p>CMake generates a macro for the CPU and TPU models; As well as the models/ subfolder:</p> <p><code>CFML_FACE_DETECTOR_CPU_MODEL_PATH</code>: Points to \"models/CPU/face_detection.tflite\"</p> <p><code>CFML_FACE_DETECTOR_TPU_MODEL_PATH</code>: Points to \"models/TPU/face_detection.tflite\"</p> <p><code>CFML_FACE_DETECTOR_MODEL_DIR</code>: Points to \"models/\" folder of this library</p> <p>Number of threads</p> <p>Number of threads indicate how many CPU threads to use for the inference delegate. </p> <p>Please note that this can only be used with the CPU delegate!. When using the TPU delegate, this parameter will be ignored. As adding threads with TPU adds aditional latency as the threads have to wait for synchronisation everytime one thread offloads it's task to the same TPU.</p> <p>Alignment of Face ROI</p> <p>Please note that the Face ROI is top-left aligned. </p>"},{"location":"usage/overview/#cmake-integration","title":"CMake integration","text":"<p>This project uses CMake for generating the build files. The CMakeLists is configured as follows:</p>"},{"location":"usage/overview/#targets","title":"Target(s)","text":"<p>The main target defined in the CMakeLists is the <code>face_detector</code> target. As this will not be the only library released under the CLFML organisation, we chose to namespace it and call it <code>CLFML::face_detector</code>. </p> <p>Other targets which are defined in the CMake files of this project are the Unit tests.</p> <p>Unit Tests are not built by default!</p> <p>Unit tests are not built by default! To build the unit-tests, pass in the <code>-DCLFML_FACE_DETECTOR_BUILD_UNIT_TESTS</code> argument to the CMake generator.</p> <p>e.g. <code>cmake -B build -DCLFML_FACE_DETECTOR_BUILD_UNIT_TESTS</code></p>"},{"location":"usage/overview/#configuration-options","title":"Configuration options","text":"<p>Some of the configuration options which can be used to generate the CMake project are:</p> <ul> <li> <p><code>CLFML_FACE_DETECTOR_BUILD_UNIT_TESTS</code>; Build Unit tests (Default=OFF)</p> </li> <li> <p><code>CLFML_FACE_DETECTOR_BUILD_EXAMPLE_PROJECTS</code>; Build example projects (Face roi demo) (Default=ON, *only when project is not part of other project)</p> </li> <li> <p><code>CLFML_FACE_DETECTOR_ENABLE_CORAL_TPU</code>; Enables Coral TPU support (Default=OFF)</p> </li> <li> <p><code>CLFML_ROS2_PACKAGE_BUILD</code>; Build a ROS2 (Default=OFF, *Note that colcon is used as the build system.) </p> </li> </ul>"},{"location":"usage/overview/#integrating-it-into-your-own-project","title":"Integrating it into your own project","text":"<p>Here are some CMake snippets which indicate how this project might be included into your own CMake project.</p> <p>Automatically fetching from GitHub</p> <p>CPU only: <pre><code>include(FetchContent)\n\nFetchContent_Declare(\n Face_Detector.Cpp\n GIT_REPOSITORY https://github.com/CLFML/Face_Detector.Cpp.git\n GIT_TAG        main\n)\nFetchContent_MakeAvailable(Face_Detector.Cpp)\n\n...\n\ntarget_link_libraries(YOUR_MAIN_EXECUTABLE_NAME CLFML::face_detector)\n</code></pre></p> <p>TPU and CPU (requires version matched libedgetpu to be installed on your system *): <pre><code>include(FetchContent)\nset(CLFML_FACE_DETECTOR_ENABLE_CORAL_TPU ON CACHE INTERNAL \"\")  # Makes sure that the lib will build with TPU support\n\nFetchContent_Declare(\n Face_Detector.Cpp\n GIT_REPOSITORY https://github.com/CLFML/Face_Detector.Cpp.git\n GIT_TAG        main\n)\nFetchContent_MakeAvailable(Face_Detector.Cpp)\n\n...\n\ntarget_link_libraries(YOUR_MAIN_EXECUTABLE_NAME CLFML::face_detector)\n</code></pre></p> <p>*Version matched with the tensorflow used by this lib (currenly 1.16.1)</p> <p>Manually using add_subdirectory</p> <p>First make sure that this library is cloned into the project directory!     CPU only: <pre><code>add_subdirectory(Face_Detector.Cpp)\n...\n\ntarget_link_libraries(YOUR_MAIN_EXECUTABLE_NAME CLFML::face_detector)\n</code></pre></p> <p>TPU and CPU (requires version matched libedgetpu to be installed on your system *): <pre><code>set(CLFML_FACE_DETECTOR_ENABLE_CORAL_TPU ON CACHE INTERNAL \"\")  # Makes sure that the lib will build with TPU support\nadd_subdirectory(Face_Detector.Cpp)\n...\n\ntarget_link_libraries(YOUR_MAIN_EXECUTABLE_NAME CLFML::face_detector)\n</code></pre></p> <p>*Version matched with the tensorflow used by this lib (currenly 1.16.1)</p>"},{"location":"usage/build_environment/usage_with_linux/","title":"Set-up build environment on Linux","text":"<p>Linux is one of the easiest os'es to set-up as most packages and libraries can be found in the package repositories.</p>"},{"location":"usage/build_environment/usage_with_linux/#ubuntu-and-debian","title":"Ubuntu and Debian","text":"<pre><code>sudo apt-get update &amp;&amp; sudo apt-get install build-essential cmake ninja-build libopencv-dev\n</code></pre> <p>If you have not installed VSCode yet, do not install using APT in ubuntu as it will install the sandboxed snap version.</p> <p>Which has many issues due to the sandbox environment</p> <p>Use this guide instead, which installs it using the APT repository from Microsoft themselves.</p>"},{"location":"usage/build_environment/usage_with_linux/#arch","title":"Arch","text":"<pre><code>sudo pacman -S opencv cmake gcc ninja\n</code></pre> <p>If you have not installed VSCode yet,</p> <p>Install the <code>visual-studio-code-bin</code> package from AUR.</p>"},{"location":"usage/build_environment/usage_with_linux/#fedora","title":"Fedora","text":"<pre><code>sudo dnf install opencv-devel gcc cmake ninja\n</code></pre> <p>If you have not installed VSCode yet, use this guide.</p>"},{"location":"usage/build_environment/usage_with_linux/#compiling-and-running-the-example","title":"Compiling and running the example","text":"<p>The library contains an example demonstrating the usage and functionality of this library. </p> <p>To compile and run this example:</p> <ol> <li> <p>Clone this repo: <pre><code>git clone https://github.com/CLFML/Face_Detector.Cpp.git\n</code></pre></p> </li> <li> <p>Open the cloned repo folder in vscode; <code>File-&gt;Open Folder</code></p> </li> <li> <p>Select Ninja as build generator by pressing CRTL+SHIFT+P-&gt;\"CMake: Open CMake Tools Extension Settings\"-&gt;\"@ext:ms-vscode.cmake-tools generator\"    Now type Ninja (with capital N into the generator field!).    </p> </li> <li> <p>Select the <code>GCC kit</code>by pressing CTRL+SHIFT+p and selecting <code>CMake: Select a kit</code>.    </p> </li> <li> <p>CMake will now configure; By default it will configure as Debug build, this has a significant performance hit.    To change to release with debug info (which has optimizations turned on, but is still debuggable). Press CTRL+SHIFT+p again and enter <code>CMake: Select Variant</code>-&gt; <code>RelWithDebInfo</code> </p> </li> <li> <p>Let CMake Finish configuring your build configuration. Then click on the Play button on the blue bar on the bottom of screen, CMake might ask which target to launch, select the <code>Face_roi_demo</code> target.    </p> </li> <li> <p>After build is finished, it will launch the demo which uses your camera to detect your face.</p> </li> </ol>"},{"location":"usage/build_environment/usage_with_mac/","title":"Set-up build environment on Mac Os","text":"<p>For Mac os it is easier to use Brew (community-based package manager). </p> <p>With Brew installed it is relatively easy as it requires one command to install all the required packages and libraries.</p>"},{"location":"usage/build_environment/usage_with_mac/#installing-brew","title":"Installing Brew","text":"<p>Follow the instructions on the webpage.</p>"},{"location":"usage/build_environment/usage_with_mac/#installation-of-packages-and-libraries","title":"Installation of packages and libraries","text":"<p>The command for installing the packages and libraries:</p> <pre><code>brew install cmake gcc opencv ninja\n</code></pre>"},{"location":"usage/build_environment/usage_with_mac/#installing-vscode","title":"Installing VSCode","text":"<p>Follow the instructions on the VSCode website.</p>"},{"location":"usage/build_environment/usage_with_mac/#compiling-and-running-the-example","title":"Compiling and running the example","text":"<p>The library contains an example demonstrating the usage and functionality of this library. </p> <p>To compile and run this example:</p> <ol> <li> <p>Clone this repo: <pre><code>git clone https://github.com/CLFML/Face_Detector.Cpp.git\n</code></pre></p> </li> <li> <p>Open the cloned repo folder in vscode; <code>File-&gt;Open Folder</code></p> </li> <li> <p>Select Ninja as build generator by pressing CRTL+SHIFT+P-&gt;\"CMake: Open CMake Tools Extension Settings\"-&gt;\"@ext:ms-vscode.cmake-tools generator\"    Now type Ninja (with capital N into the generator field!).    </p> </li> <li> <p>Select the <code>GCC kit</code>by pressing CTRL+SHIFT+p and selecting <code>CMake: Select a kit</code>.</p> </li> <li> <p>CMake will now configure; By default it will configure as Debug build, this has a significant performance hit.    To change to release with debug info (which has optimizations turned on, but is still debuggable). Press CTRL+SHIFT+p again and enter <code>CMake: Select Variant</code>-&gt; <code>RelWithDebInfo</code> </p> </li> <li> <p>Let CMake Finish configuring your build configuration. Then click on the Play button on the blue bar on the bottom of screen, CMake might ask which target to launch, select the <code>Face_roi_demo</code> target.    </p> </li> <li> <p>After build is finished, it will launch the demo which uses your camera to detect your face.</p> </li> </ol>"},{"location":"usage/build_environment/usage_with_windows/","title":"Set-up build environment on Windows","text":"<p>Before you can compile and run the demo you need to install the following tools and libraries:</p> <ul> <li>OpenCV</li> <li>Any C/C++ compiler (MSVC is recommended, but GCC works as well)</li> <li>CMake</li> <li>Ninja (Optional, but recommended as generating for MSBuild is very slow!)</li> <li>Any code editor (This guide will use VSCode, as it is free and easy to configure with CMake)</li> </ul> <p>Note</p> <p>Although any recent enough C/C++ compiler can be used. </p> <p>This guide will only use the MSVC compiler!</p> <p>This choice was made as the precompiled versions of OpenCV are easier to configure with MSVC.</p> <p>Warning</p> <p>Due to compatibility with OpenCV and other libraries, it is recommended to use MSVC16 (Visual studio 2019 edition). </p> <p>As the newest MSVC(19, Visual studio 2022) is still not supported by the precompiled libraries of OpenCV (and many other libraries).</p>"},{"location":"usage/build_environment/usage_with_windows/#installing-msvc16-2019-edition-cmake","title":"Installing MSVC16 (2019 edition) &amp; CMake","text":"<p>The MSVC compiler (and CMake) can be installed by either installing VS BuildTools or Visual Studio 2019.</p> <p>This guide will use the VS BuildTools method, as we don't need the Visual Studio IDE.</p> <p>There are multiple ways you can download and install VS BuildTools 2019;</p> <ul> <li>Manually downloading and installing using this link</li> <li>Using chocolatey:   <code>choco install visualstudio2019buildtools</code></li> <li>Using winget: <code>winget install --id=Microsoft.VisualStudio.2019.BuildTools  -e</code></li> </ul>"},{"location":"usage/build_environment/usage_with_windows/#manually-installing-vs-build-tools-2019","title":"Manually installing VS Build Tools 2019","text":"<ol> <li>Download and run this installer.</li> <li>Select these options: </li> <li>After installation, reboot your machine!</li> </ol>"},{"location":"usage/build_environment/usage_with_windows/#installing-ninja","title":"Installing Ninja","text":"<ol> <li>Download the latest Ninja release for Windows!</li> <li>Unzip this ninja-win.zip to <code>C:\\ninja-win</code></li> <li>Open the environment variables editor using the Windows Startup Menu (Try this guide if you can't find it)</li> <li>Add the <code>C:\\ninja-win</code> path to the PATH variable;</li> <li>Open a commandline window and check if Ninja is correctly installed by running the <code>ninja</code> command!</li> </ol>"},{"location":"usage/build_environment/usage_with_windows/#installing-opencv","title":"Installing OpenCV","text":"<p>OpenCV offers precompiled libraries for usage with the MSVC compiler on Windows. To install these libraries;</p> <p>Warning</p> <p>It is recommended to leave the installation path to the default setting;</p> <p>This path needs to be added to environment variables and your PATH variable! </p> <p>If you still want to pursue with a custom installation path,</p> <p>Change the default; \"C:\\OpenCV\" path in STEP 3 &amp; 4 to your custom installation path!</p> <ol> <li>Download and install the latest OpenCV for Windows Release!</li> <li>Open the environment variables editor using the Windows Startup Menu (Try this guide if you can't find it)</li> <li>Add this variable: <code>OPENCV_DIR</code> with value: <code>C:\\OpenCV\\build\\VC16</code></li> <li>Add this to the Path variable: <code>C:\\OpenCV\\bin</code></li> <li>Reboot you machine</li> </ol>"},{"location":"usage/build_environment/usage_with_windows/#installing-vscode-with-plugins","title":"Installing VSCode (with plugins)","text":"<p>VSCode is an easy to use code-editor with CMake support (using the CMake Tools plugin). </p> <p>To set-up VSCode the follow these steps:</p> <ol> <li>Download and install VSCode using the installer</li> <li>Follow the initial set-up wizard in vscode (if freshly installed)</li> <li>Download and install this plugin pack:<ul> <li>C/C++ Extension Pack (Microsoft)</li> </ul> </li> </ol>"},{"location":"usage/build_environment/usage_with_windows/#compiling-and-running-the-example","title":"Compiling and running the example","text":"<p>The library contains an example demonstrating the usage and functionality of this library. </p> <p>To compile and run this example:</p> <ol> <li> <p>Clone this repo: <pre><code>git clone https://github.com/CLFML/Face_Detector.Cpp.git\n</code></pre></p> </li> <li> <p>Open the cloned repo folder in vscode; <code>File-&gt;Open Folder</code></p> </li> <li> <p>Select Ninja as build generator by pressing CRTL+SHIFT+P-&gt;\"CMake: Open CMake Tools Extension Settings\"-&gt;\"@ext:ms-vscode.cmake-tools generator\"    Now type Ninja (with capital N into the generator field!).    </p> </li> <li> <p>Select the <code>MSVC x86_amd64 kit</code>by pressing CTRL+SHIFT+p and selecting <code>CMake: Select a kit</code>.</p> </li> <li> <p>CMake will now configure; By default it will configure as Debug build, this has a significant performance hit.    To change to release with debug info (which has optimizations turned on, but is still debuggable). Press CTRL+SHIFT+p again and enter <code>CMake: Select Variant</code>-&gt; <code>RelWithDebInfo</code> </p> </li> <li> <p>Let CMake Finish configuring your build configuration. Then click on the Play button on the blue bar on the bottom of screen, CMake might ask which target to launch, select the <code>Face_roi_demo</code> target.    </p> </li> <li> <p>After build is finished, it will launch the demo which uses your camera to detect your face.</p> </li> </ol>"},{"location":"usage/ros2_build_environment/usage_with_linux/","title":"Set-up build environment on Linux","text":"<p>Linux is one of the easiest os'es to set-up as most packages and libraries can be found in the package repositories.</p>"},{"location":"usage/ros2_build_environment/usage_with_linux/#ubuntu-and-debian","title":"Ubuntu and Debian","text":"<p>We currently support Ubuntu Noble (24.04) 64-bit x86 and 64-bit ARM.</p> <p>Use this guide to install ROS 2 Jazzy Jalisco on your system.</p> <p>Make sure ros cv bridge is installed as well,</p> <pre><code>sudo apt install ros-jazzy-cv-bridge\n</code></pre> <p>If you have not installed VSCode yet, do not install using APT in ubuntu as it will install the sandboxed snap version.</p> <p>Which has many issues due to the sandbox environment</p> <p>Use this guide instead, which installs it using the APT repository from Microsoft themselves.</p>"},{"location":"usage/ros2_build_environment/usage_with_linux/#compiling-and-running-the-example","title":"Compiling and running the example","text":"<p>The library contains a ROS2 package example demonstrating the usage and functionality of this library. </p> <p>To compile and run this example:</p> <ol> <li> <p>Clone this repo: <pre><code>git clone https://github.com/CLFML/Face_Detector.Cpp.git\n</code></pre></p> </li> <li> <p>Open the cloned repo folder in a terminal</p> </li> <li> <p>Source your ROS2 installation:</p> </li> </ol> <pre><code>source /opt/ros/jazzy/setup.bash\n</code></pre> <ol> <li> <p>Install the ros2 dependencies: <pre><code>rosdep install --from-paths src -y --ignore-src\n</code></pre></p> </li> <li> <p>Build the package with more verbose output</p> </li> </ol> <pre><code>colcon build --event-handlers console_direct+ --packages-select face_detector\n</code></pre> <p>Note that, whenever build errors occur, and you need to clean the build, use</p> <pre><code>rm -rf build install log\nrm -rf ~/.ros\n</code></pre> <ol> <li>Set up the environment</li> </ol> <pre><code>source install/setup.bash\n</code></pre> <ol> <li>With the environment sourced, we can run executables built by colcon. Let\u2019s run the camera node from the examples:</li> </ol> <pre><code>ros2 run v4l2_camera v4l2_camera_node\n</code></pre> <ol> <li> <p>In another terminal, let\u2019s run the face detector node (don\u2019t forget to source the setup script):6 <pre><code>ros2 run face_detector face_detector_node\n</code></pre></p> </li> <li> <p>In yet another terminal, let\u2019s run the viewer node:</p> </li> </ol> <pre><code>ros2 run face_detector face_detector_viewer\n</code></pre> <p>Now, you should see the camera stream, with annotations produced by the face detector.</p>"},{"location":"usage/tpu/overview/","title":"Overview","text":"<p>Using a TPU can greatly improve performance, from our limited testing we got a 2-3x decrease in CPU utilization. TPU support is build in to our library and can be enabled by setting the <code>CLFML_FACE_DETECTOR_ENABLE_CORAL_TPU</code> to ON during the CMake Generation step like this;</p> <pre><code>cmake . -B build -DCLFML_FACE_DETECTOR_ENABLE_CORAL_TPU=ON -G Ninja\n</code></pre> <p>Before you build this library with Coral support turned on; You need to compile libedgetpu for your platform:</p> <ul> <li>Linux x86_64</li> <li>Linux ARM64</li> <li>Windows x86_64</li> </ul>"},{"location":"usage/tpu/overview/#hardware","title":"Hardware","text":"<p>This library supports the PCIe aswell as the USB accelerators. For prices and stock, take a look at the coral website.</p>"},{"location":"usage/tpu/using_a_tpu_on_arm64_linux/","title":"TPU usage on Linux (ARM64)","text":""},{"location":"usage/tpu/using_a_tpu_on_arm64_linux/#wip","title":"WIP","text":""},{"location":"usage/tpu/using_a_tpu_on_x86_64_linux/","title":"TPU usage on Linux (x86_64)","text":"<p>Getting a Coral TPU might be a bit challenging to setup. The challenge is mainly in getting the right version of the libedgetpu library to work with our TensorFlow version. The rest is a lot easier as it is just one argument to compile with edgetpu support enabled.</p>"},{"location":"usage/tpu/using_a_tpu_on_x86_64_linux/#getting-libedgetpu-to-work","title":"Getting libedgetpu to work!","text":"<p>Libedgetpu is version tied to the Tensorflow lite version which we are using! Which is TensorFlow Lite 2.16.1; Which is always a bit outdated as there are already way newer versions of Tensorflow around. Another annoyance is that the packages from the APT, RPM and AUR are either out-of-date or pulled from the package repository's. Meaning that to install the library we cannot make use of precompiled libraries and have to compile it ourselfs.</p>"},{"location":"usage/tpu/using_a_tpu_on_x86_64_linux/#compiling-libedgetpu","title":"Compiling libedgetpu","text":"<p>Compiling libedgetpu is annoying, mainly do to the bazel buildsystem (which is Google's own build-system). Their buildscripts require a quite recent version of bazel, which can't be found in your APT and RPM repository's (in rolling distro's such as ARCH this is not a problem!). So to install these packages you run:</p> <p>On Debian &amp; Ubuntu*;</p> <p><pre><code>sudo apt-get remove bazel-bootstrap -y &amp;&amp;\nsudo apt install apt-transport-https curl gnupg -y\ncurl -fsSL https://bazel.build/bazel-release.pub.gpg | gpg --dearmor &gt;bazel-archive-keyring.gpg\nsudo mv bazel-archive-keyring.gpg /usr/share/keyrings\necho \"deb [arch=amd64 signed-by=/usr/share/keyrings/bazel-archive-keyring.gpg] https://storage.googleapis.com/bazel-apt stable jdk1.8\" | sudo tee /etc/apt/sources.list.d/bazel.list &amp;&amp; sudo apt update &amp;&amp; sudo apt -y install bazel make libusb-1.0-0.dev devscripts\n</code></pre> On ubuntu install this package too, as it is not included by default and is needed to package up the .deb file:</p> <pre><code>sudo apt-get install dh-make\n</code></pre> <p>On Fedora; <pre><code>dnf copr enable vbatts/bazel\ndnf install bazel make\n</code></pre></p> <p>On Suse; <pre><code>zypper install bazel make\n</code></pre></p> <p>Then clone the libedgetpu repository</p> <pre><code>git clone https://github.com/google-coral/libedgetpu.git\n</code></pre> <p>And compile:</p> <pre><code>cd libedgetpu &amp;&amp; make\n</code></pre> <p>This should take around 5-10 minutes on a moderately fast computer (i5 10th gen). </p> <p>Now after compilation you should add these libraries and include files to your linux filesystem.</p> <p>On Debian &amp; Ubuntu based distro's:</p> <p><pre><code>debuild -us -uc -tc -b -d &amp;&amp; cd ..\n</code></pre> After which you get two deb files; libedgetpu-std-... and libedgetpu-max-...</p> <p>The std underclocks the edgetpu so that it does not get hot. The max runs the edgetpu at maximum speed.</p> <p>Install the std version: <pre><code>sudo dpkg -i libedgetpu1-std*.deb libedgetpu-dev*.deb\n</code></pre></p> <p>For Fedora and other RPM-based distro's follow this guide</p>"},{"location":"usage/tpu/using_a_tpu_on_x86_64_windows/","title":"TPU usage on Windows (x86_64)","text":""},{"location":"usage/tpu/using_a_tpu_on_x86_64_windows/#wip","title":"WIP","text":""}]}